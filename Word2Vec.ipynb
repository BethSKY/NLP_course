{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tb3vk2XeKtT"
      },
      "source": [
        "# <font color=purple>Trying to understand how Word2Vec works</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHx3xnSATylQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bef5220-45a8-4e2a-980e-0623ec00aa63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "import gensim.downloader as api\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
        "import logging "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords') #<-- we run this command to download the stopwords in the project\n",
        "nltk.download('punkt') #<-- essential for tokenization\n",
        "nltk.download('gutenberg') #<-- corpus for training the model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_qp_B4XBkl-",
        "outputId": "c87fffe6-4f2e-44de-d526-94cae79422c0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "T7CLM5f2BLTx"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import itertools\n",
        "import math\n",
        "import string\n",
        "import nltk\n",
        "# Access the Gutenberg Corpus\n",
        "from nltk.corpus import gutenberg\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ku-rSdWzhn3S"
      },
      "source": [
        "# Function from lecturer\n",
        "## <font color=purple>The function that cleans the data</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "BveB75-ThpA4"
      },
      "outputs": [],
      "source": [
        "def clean(inp: str) -> str:\n",
        "\n",
        "    inp = inp.translate(str.maketrans(string.punctuation, \" \"*len(string.punctuation)))\n",
        "    inp = re.sub(r'\\s+', ' ', inp.lower())\n",
        "\n",
        "    return inp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 208,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "IQS0IwTdhuB9",
        "outputId": "bcf3d12e-c16d-4490-fc3c-48adb304bf6e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' if you use a car frequently the first step to cutting down your emissions may well be to simply fully consider the alternatives available to you '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 208
        }
      ],
      "source": [
        "# from a list of sentences\n",
        "documents = [\"If you use a car frequently, the first step to cutting\",\n",
        "             \"down your emissions may well be to simply\", \n",
        "             \"fully consider the\", \n",
        "             \"alternatives available to you.\"\n",
        "             ]\n",
        "clean(str(documents))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIXz6PtHhfjN"
      },
      "source": [
        "# <font color=purple>To train my own Word2Vec model I need:</font>\n",
        "1. **Obtain a large corpus of text**: You will need a large amount of text data to train your Word2vec model. You can obtain text data from various sources such as Wikipedia, news articles, or social media posts.\n",
        "2. **Preprocess the text**: Before training the Word2vec model, you will need to preprocess the text data by removing stop words, punctuations, and other non-essential elements. You can also tokenize the text data into words or phrases to prepare it for training.\n",
        "3. **Choose a Word2vec algorithm**: There are two main algorithms for training Word2vec models: Continuous Bag of Words (CBOW) and Skip-gram. CBOW predicts a word based on its context, while Skip-gram predicts the context based on a word. Choose the algorithm that best suits your needs.\n",
        "4. **Train the Word2vec model**: You can train the Word2vec model using popular libraries such as Gensim or TensorFlow. These libraries provide easy-to-use functions to train and test the Word2vec model. During training, the model learns to associate each word in the vocabulary with a vector of real numbers, which represent the word embedding.\n",
        "5. **Evaluate the Word2vec model**: Once the Word2vec model is trained, you can evaluate its performance using intrinsic or extrinsic evaluation methods. Intrinsic evaluation focuses on evaluating specific aspects of the model in isolation, such as its ability to generate embeddings or to classify sentences based on sentiment. Extrinsic evaluation measures the performance of the model on a specific task, such as language modeling or sentiment analysis.\n",
        "6. **Use the Word2vec model**: After training and evaluating the Word2vec model, you can use it for various natural language processing tasks, such as information retrieval, text classification, or machine translation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdWXu_HUkrJC"
      },
      "source": [
        "## <font color=blue>Obtain a large corpus of text</font>\n",
        "> Попробуем сначала поработать с корпусом Gutenberg (классическая литература), если что, переключимся на Brown (новости на английском)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIgUJOK4kx0M",
        "outputId": "9ad31638-c913-4698-c26a-b0b1c1a320b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt'] \n",
            "\n",
            "['[', 'Alice', \"'\", 's', 'Adventures', 'in', 'Wonderland', 'by', 'Lewis', 'Carroll', '1865', ']']\n",
            "['CHAPTER', 'I', '.']\n",
            "['Down', 'the', 'Rabbit', '-', 'Hole']\n",
            "['Alice', 'was', 'beginning', 'to', 'get', 'very', 'tired', 'of', 'sitting', 'by', 'her', 'sister', 'on', 'the', 'bank', ',', 'and', 'of', 'having', 'nothing', 'to', 'do', ':', 'once', 'or', 'twice', 'she', 'had', 'peeped', 'into', 'the', 'book', 'her', 'sister', 'was', 'reading', ',', 'but', 'it', 'had', 'no', 'pictures', 'or', 'conversations', 'in', 'it', ',', \"'\", 'and', 'what', 'is', 'the', 'use', 'of', 'a', 'book', \",'\", 'thought', 'Alice', \"'\", 'without', 'pictures', 'or', 'conversation', \"?'\"]\n",
            "['So', 'she', 'was', 'considering', 'in', 'her', 'own', 'mind', '(', 'as', 'well', 'as', 'she', 'could', ',', 'for', 'the', 'hot', 'day', 'made', 'her', 'feel', 'very', 'sleepy', 'and', 'stupid', '),', 'whether', 'the', 'pleasure', 'of', 'making', 'a', 'daisy', '-', 'chain', 'would', 'be', 'worth', 'the', 'trouble', 'of', 'getting', 'up', 'and', 'picking', 'the', 'daisies', ',', 'when', 'suddenly', 'a', 'White', 'Rabbit', 'with', 'pink', 'eyes', 'ran', 'close', 'by', 'her', '.']\n",
            "['[', 'Alice', \"'\", 's', 'Adventures', 'in', 'Wonderland', 'by', 'Lewis', 'Carroll', '1865', ']', 'CHAPTER', 'I', '.', 'Down', 'the', 'Rabbit', '-', 'Hole']\n"
          ]
        }
      ],
      "source": [
        "# Print the categories in the Brown Corpus\n",
        "print(gutenberg.fileids(), '\\n')\n",
        "\n",
        "# Print the first few sentences of a book\n",
        "sentences = gutenberg.sents('carroll-alice.txt')\n",
        "for sentence in sentences[:5]:\n",
        "    print(sentence)\n",
        "\n",
        "# Transform data from nested lists to one list\n",
        "result1 = list(itertools.chain(*sentences))\n",
        "print(result1[:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjKWptxrqnbO"
      },
      "source": [
        "## <font color=blue>Preprocess the text</font>\n",
        "\n",
        "Source: https://towardsdatascience.com/how-to-train-a-word2vec-model-from-scratch-with-gensim-c457d587e031"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 275,
      "metadata": {
        "id": "47T2Jo2GlKQu"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text: str) -> str:\n",
        "    \"\"\"Function that cleans the input text by going to:\n",
        "    - remove links\n",
        "    - remove special characters\n",
        "    - remove numbers\n",
        "    - remove stopwords\n",
        "    - convert to lowercase\n",
        "    - remove excessive white spaces\n",
        "    Arguments:\n",
        "        text (str): text to clean\n",
        "        remove_stopwords (bool): whether to remove stopwords\n",
        "    Returns:\n",
        "        str: cleaned text\n",
        "    \"\"\"\n",
        "    # remove links\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)\n",
        "    # remove numbers and special characters\n",
        "    text = re.sub(\"[^A-Za-z]+\", \" \", text)\n",
        "    # remove stopwords\n",
        "    # 1. create tokens\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    # 2. check if it's a stopword\n",
        "    tokens = [w.lower().strip() for w in tokens if not w.lower() in stopwords.words(\"english\")]\n",
        "    # return a list of cleaned tokens\n",
        "    return [tokens]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts = preprocess_text(clean(str(documents)))\n",
        "texts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acFgAs2Cz3ao",
        "outputId": "8de2f4dd-d686-495a-d60e-99b1af4328ea"
      },
      "execution_count": 276,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['use',\n",
              "  'car',\n",
              "  'frequently',\n",
              "  'first',\n",
              "  'step',\n",
              "  'cutting',\n",
              "  'emissions',\n",
              "  'may',\n",
              "  'well',\n",
              "  'simply',\n",
              "  'fully',\n",
              "  'consider',\n",
              "  'alternatives',\n",
              "  'available']]"
            ]
          },
          "metadata": {},
          "execution_count": 276
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color=blue>Train the Word2Vec model</font>"
      ],
      "metadata": {
        "id": "gX40D4TRq0Yc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color=blue>Convert to one function</font>"
      ],
      "metadata": {
        "id": "b1Aowb5bB6s0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "def tokenize(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    # Split text into tokens (words)\n",
        "    tokens = text.split()\n",
        "    return [tokens]"
      ],
      "metadata": {
        "id": "DxXq2e7Cnx6w"
      },
      "execution_count": 282,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vocab(sentences):\n",
        "    words = []\n",
        "    for sentence in sentences:\n",
        "        words.extend(sentence)\n",
        "    words = sorted(set(words))\n",
        "    word2id = {w: i for i, w in enumerate(words)}\n",
        "    id2word = {i: w for w, i in word2id.items()}\n",
        "    return word2id, id2word"
      ],
      "metadata": {
        "id": "0OVrsw5KHNTK"
      },
      "execution_count": 247,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def skipgrams(sentence, window_size):\n",
        "    pairs = []\n",
        "    for i, w in enumerate(sentence):\n",
        "        for j in range(i - window_size, i + window_size + 1):\n",
        "            if j != i and j >= 0 and j < len(sentence):\n",
        "                pairs.append((w, sentence[j]))\n",
        "    return pairs"
      ],
      "metadata": {
        "id": "F52Vt0ftnyMu"
      },
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_weights(vocabulary_size, vector_size):\n",
        "    W1 = np.random.uniform(-0.5, 0.5, size=(vocabulary_size, vector_size))\n",
        "    W2 = np.random.uniform(-0.5, 0.5, size=(vocabulary_size, vector_size))\n",
        "    return W1, W2"
      ],
      "metadata": {
        "id": "uNzUT5T-nyKJ"
      },
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pred_function(x):\n",
        "    exp_scores = np.exp(x - np.max(x))\n",
        "    return exp_scores / np.sum(exp_scores, axis=0)"
      ],
      "metadata": {
        "id": "VWtJRKmCnyHx"
      },
      "execution_count": 286,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_weights(W1, W2, target_word_idx, context_words, dL_dh, dL_du, learning_rate):\n",
        "    W2 -= learning_rate * np.outer(W1[target_word_idx], dL_du)\n",
        "    W1[target_word_idx] -= learning_rate * dL_dh"
      ],
      "metadata": {
        "id": "l1no5CSznyFL"
      },
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_old(pairs, word2id, W1, W2, learning_rate, epochs, vector_size):\n",
        "    for epoch in range(epochs):\n",
        "        loss_history = []\n",
        "        loss = 0\n",
        "        for pair in pairs:\n",
        "            center_word = np.zeros((vector_size,))\n",
        "            context_word = np.zeros((vector_size,))\n",
        "            u = word2id[pair[0]]\n",
        "            v = word2id[pair[1]]\n",
        "            center_word = W1[u]\n",
        "            for context in range(len(word2id)):\n",
        "                if context == v:\n",
        "                    context_word = W2[context]\n",
        "                    z = np.dot(center_word, context_word)\n",
        "                    sig = pred_function(z)\n",
        "                    e = (1 - int(context == v)) - sig\n",
        "                    loss += e**2\n",
        "                    grad_sig = e * learning_rate\n",
        "                    grad_context_word = center_word * grad_sig\n",
        "                    grad_center_word = context_word * grad_sig\n",
        "                    W1[u] -= grad_center_word\n",
        "                    W2[context] -= grad_context_word\n",
        "        # loss_history.append(loss / len(pairs))\n",
        "    return W1, W2"
      ],
      "metadata": {
        "id": "dml5rnMwnyCk"
      },
      "execution_count": 205,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(data: str):\n",
        "    # All parameters for training the Word2Vec model\n",
        "    window_size=2\n",
        "    vector_size=10\n",
        "    learning_rate=0.025\n",
        "    epochs=10\n",
        "\n",
        "    # Create tokens\n",
        "    text = tokenize(data)\n",
        "\n",
        "    # Build vocabulary\n",
        "    word2id, id2word = build_vocab(text)\n",
        "\n",
        "    # Generate skip-grams\n",
        "    pairs = []\n",
        "    for sentence in data:\n",
        "        pairs.extend(skipgrams(sentence, window_size))\n",
        "\n",
        "    # Initialize weights\n",
        "    W, W_context = initialize_weights(len(word2id), vector_size)\n",
        "\n",
        "    # Train model\n",
        "    W, W_context = train_old(pairs, word2id, W, W_context, learning_rate, epochs, vector_size)\n",
        "\n",
        "    # Create final dictionary\n",
        "    dict_final = {key: W[word2id[key]] for key in word2id.keys()}\n",
        "\n",
        "    return dict_final"
      ],
      "metadata": {
        "id": "Df2HqKAfnx_9"
      },
      "execution_count": 287,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(clean(str(documents)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ps0X37ZHnx9g",
        "outputId": "422cb784-f200-4b76-8902-69bb29bd8ff7"
      },
      "execution_count": 288,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'a': array([ 0.28847533, -0.42579127, -0.49083777,  0.39821145,  0.23358213,\n",
              "         0.10700838, -0.17352341, -0.38870176,  0.38830678, -0.46624828]),\n",
              " 'alternatives': array([-0.00880526,  0.27288321, -0.07149046,  0.10515103,  0.44149732,\n",
              "        -0.1644551 ,  0.25820854,  0.30934356, -0.19587371, -0.1658254 ]),\n",
              " 'available': array([-0.45579346, -0.21392973, -0.10299653, -0.45210641, -0.21152857,\n",
              "         0.09706983, -0.39095764,  0.36747191,  0.46069989, -0.1678274 ]),\n",
              " 'be': array([-0.07733037, -0.20770594, -0.1447155 , -0.28522312, -0.29327728,\n",
              "        -0.11703954, -0.38305983, -0.2406282 , -0.14250838, -0.46011462]),\n",
              " 'car': array([-0.23785869,  0.18214342, -0.40325524, -0.07496411, -0.43008292,\n",
              "        -0.04985569, -0.16676924, -0.29822325, -0.32330982,  0.11061734]),\n",
              " 'consider': array([-0.10587109, -0.24405389,  0.08443648, -0.08020798,  0.37298265,\n",
              "         0.3366705 ,  0.49947678, -0.02671674,  0.18346326, -0.29558107]),\n",
              " 'cutting': array([-0.20145565, -0.38432145, -0.18997209, -0.08574821,  0.21169889,\n",
              "        -0.00723457,  0.31459399, -0.13577315, -0.26456196, -0.08863511]),\n",
              " 'down': array([ 0.44042948, -0.49789591, -0.22198782,  0.38322236, -0.24909394,\n",
              "        -0.11235552,  0.02332715, -0.30375027,  0.46462323,  0.12567505]),\n",
              " 'emissions': array([-0.49784423, -0.14043313, -0.00641709,  0.39137825,  0.48614651,\n",
              "        -0.05479719,  0.41576618, -0.24973063, -0.4224489 , -0.02640925]),\n",
              " 'first': array([ 0.1194518 ,  0.26321361, -0.28512266, -0.24218195,  0.18114036,\n",
              "         0.41935811,  0.42376263, -0.25432839,  0.07040304,  0.15612244]),\n",
              " 'frequently': array([-0.43516173,  0.2653549 ,  0.2365209 , -0.49800776, -0.09492881,\n",
              "         0.43843251, -0.17659675, -0.28137386,  0.44137167, -0.08588435]),\n",
              " 'fully': array([-0.44156741,  0.03770669,  0.48836153, -0.26214907,  0.16755334,\n",
              "         0.21122282, -0.33176692,  0.33755205, -0.10263665,  0.28863946]),\n",
              " 'if': array([-0.49979378,  0.34189695,  0.29654884,  0.02480117, -0.18068324,\n",
              "         0.27232027,  0.45394033,  0.24554248, -0.11295933,  0.3696567 ]),\n",
              " 'may': array([-0.40905172,  0.44661418,  0.26504705,  0.15192719,  0.47364309,\n",
              "        -0.49038428,  0.44678204,  0.49650521,  0.07540312,  0.37919358]),\n",
              " 'simply': array([ 0.18717168, -0.20241424, -0.38268495,  0.40524243,  0.13210042,\n",
              "         0.10326376, -0.02106974, -0.49437901, -0.44311728, -0.42478904]),\n",
              " 'step': array([-0.39243754,  0.26767688, -0.26307709,  0.49254792, -0.2144181 ,\n",
              "        -0.38057279,  0.47271703,  0.39071768, -0.43807116, -0.33116873]),\n",
              " 'the': array([-0.29935347, -0.29294463,  0.24793352,  0.44538302, -0.00904771,\n",
              "        -0.21151783,  0.49823243,  0.19542812,  0.13506821, -0.06916651]),\n",
              " 'to': array([ 0.4910642 , -0.23469141,  0.46528136, -0.07623454,  0.28294545,\n",
              "         0.4601506 ,  0.0612045 ,  0.33991824,  0.46372457, -0.36207191]),\n",
              " 'use': array([ 0.40730965,  0.14855968,  0.32817836,  0.29609235, -0.11829165,\n",
              "        -0.13929227,  0.31483111,  0.05290738,  0.3127004 ,  0.33969453]),\n",
              " 'well': array([ 0.25802421, -0.26620996, -0.12404644,  0.25719772, -0.12708629,\n",
              "         0.16930588,  0.11972899, -0.08866497,  0.3934641 , -0.45107749]),\n",
              " 'you': array([-0.10401705, -0.37179233,  0.31183586,  0.39469201, -0.27128607,\n",
              "        -0.40073551, -0.214392  ,  0.35286571,  0.3229398 , -0.00816466]),\n",
              " 'your': array([-0.04345829, -0.02035629, -0.27826855, -0.34078845, -0.46234288,\n",
              "         0.09780259, -0.37757844,  0.4555207 , -0.09228411, -0.07483043])}"
            ]
          },
          "metadata": {},
          "execution_count": 288
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenize(clean(str(documents)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-sZHWHGnx3b",
        "outputId": "72a0e3d8-0e5b-49f9-cfc1-e24836a8bf02"
      },
      "execution_count": 283,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['if',\n",
              "  'you',\n",
              "  'use',\n",
              "  'a',\n",
              "  'car',\n",
              "  'frequently',\n",
              "  'the',\n",
              "  'first',\n",
              "  'step',\n",
              "  'to',\n",
              "  'cutting',\n",
              "  'down',\n",
              "  'your',\n",
              "  'emissions',\n",
              "  'may',\n",
              "  'well',\n",
              "  'be',\n",
              "  'to',\n",
              "  'simply',\n",
              "  'fully',\n",
              "  'consider',\n",
              "  'the',\n",
              "  'alternatives',\n",
              "  'available',\n",
              "  'to',\n",
              "  'you']]"
            ]
          },
          "metadata": {},
          "execution_count": 283
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jMyFBCf_nxzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OQ20t-3enxqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pGu0L8dPWK2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MLggSIMFWK0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UHtC8mMnWKyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DEYcTH9WWKwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ojsxVn6sWKuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sbFibzfbWKr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XKS8OJhMWKkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Embedings are not good enouth"
      ],
      "metadata": {
        "id": "bz_3VKIlTa8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import string\n",
        "\n",
        "def tokenize(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    # Split text into tokens (words)\n",
        "    tokens = text.split()\n",
        "    return [tokens]\n",
        "\n",
        "def build_vocab(sentences):\n",
        "    words = []\n",
        "    for sentence in sentences:\n",
        "        words.extend(sentence)\n",
        "    words = sorted(set(words))\n",
        "    word2id = {w: i for i, w in enumerate(words)}\n",
        "    id2word = {i: w for w, i in word2id.items()}\n",
        "    return word2id, id2word\n",
        "    \n",
        "def skipgrams(sentence, window_size):\n",
        "    pairs = []\n",
        "    for i, w in enumerate(sentence):\n",
        "        for j in range(i - window_size, i + window_size + 1):\n",
        "            if j != i and j >= 0 and j < len(sentence):\n",
        "                pairs.append((w, sentence[j]))\n",
        "    return pairs\n",
        "\n",
        "def initialize_weights(vocabulary_size, vector_size):\n",
        "    W1 = np.random.uniform(-0.5, 0.5, size=(vocabulary_size, vector_size))\n",
        "    W2 = np.random.uniform(-0.5, 0.5, size=(vocabulary_size, vector_size))\n",
        "    return W1, W2\n",
        "\n",
        "def pred_function(x):\n",
        "    exp_scores = np.exp(x - np.max(x))\n",
        "    return exp_scores / np.sum(exp_scores, axis=0)\n",
        "    \n",
        "def update_weights(W1, W2, target_word_idx, context_words, dL_dh, dL_du, learning_rate):\n",
        "    W2 -= learning_rate * np.outer(W1[target_word_idx], dL_du)\n",
        "    W1[target_word_idx] -= learning_rate * dL_dh\n",
        "\n",
        "def train_old(pairs, word2id, W1, W2, learning_rate, epochs, vector_size):\n",
        "    for epoch in range(epochs):\n",
        "        loss_history = []\n",
        "        loss = 0\n",
        "        for pair in pairs:\n",
        "            center_word = np.zeros((vector_size,))\n",
        "            context_word = np.zeros((vector_size,))\n",
        "            u = word2id[pair[0]]\n",
        "            v = word2id[pair[1]]\n",
        "            center_word = W1[u]\n",
        "            for context in range(len(word2id)):\n",
        "                if context == v:\n",
        "                    context_word = W2[context]\n",
        "                    z = np.dot(center_word, context_word)\n",
        "                    sig = pred_function(z)\n",
        "                    e = (1 - int(context == v)) - sig\n",
        "                    loss += e**2\n",
        "                    grad_sig = e * learning_rate\n",
        "                    grad_context_word = center_word * grad_sig\n",
        "                    grad_center_word = context_word * grad_sig\n",
        "                    W1[u] -= grad_center_word\n",
        "                    W2[context] -= grad_context_word\n",
        "        # loss_history.append(loss / len(pairs))\n",
        "    return W1, W2\n",
        "\n",
        "\n",
        "def train(data: str):\n",
        "    # All parameters for training the Word2Vec model\n",
        "    window_size=3\n",
        "    vector_size=10\n",
        "    learning_rate=0.001\n",
        "    epochs=100\n",
        "\n",
        "    # Create tokens\n",
        "    text = tokenize(data)\n",
        "\n",
        "    # Build vocabulary\n",
        "    word2id, id2word = build_vocab(text)\n",
        "\n",
        "    # Generate skip-grams\n",
        "    pairs = []\n",
        "    for sentence in data:\n",
        "        pairs.extend(skipgrams(sentence, window_size))\n",
        "\n",
        "    # Initialize weights\n",
        "    W, W_context = initialize_weights(len(word2id), vector_size)\n",
        "\n",
        "    # Train model\n",
        "    W, W_context = train_old(pairs, word2id, W, W_context, learning_rate, epochs, vector_size)\n",
        "\n",
        "    # Create final dictionary\n",
        "    dict_final = {key: W[word2id[key]] for key in word2id.keys()}\n",
        "\n",
        "    return dict_final"
      ],
      "metadata": {
        "id": "ztCX1Nf_3hXK"
      },
      "execution_count": 302,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(clean(str(documents)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpNnzSsH3nLR",
        "outputId": "8fde8192-6135-472e-c0d9-8802bc990b91"
      },
      "execution_count": 303,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'a': array([-0.36776425,  0.16565639,  0.04372147,  0.06616889,  0.38856678,\n",
              "        -0.21054818, -0.45188995,  0.43750588,  0.4791722 , -0.03311668]),\n",
              " 'alternatives': array([ 0.32914017,  0.19904258, -0.3195285 ,  0.14057175, -0.4139773 ,\n",
              "        -0.03157049,  0.13688819, -0.18928625,  0.15975498,  0.14356051]),\n",
              " 'available': array([-0.33941646,  0.07338916,  0.42723351, -0.42056585, -0.45153616,\n",
              "        -0.49481611,  0.19764023,  0.00408344,  0.4252313 , -0.08517067]),\n",
              " 'be': array([ 0.29592341,  0.34558949,  0.13111804, -0.44441846, -0.48131925,\n",
              "         0.19061378, -0.23116951, -0.45149755, -0.29856515,  0.11739898]),\n",
              " 'car': array([ 0.44380004, -0.13956756, -0.38965916,  0.03603478,  0.27824233,\n",
              "         0.0778963 ,  0.39890429, -0.39173743, -0.25234295,  0.0020075 ]),\n",
              " 'consider': array([ 0.44130905,  0.01574036,  0.41146182, -0.40065405,  0.21781645,\n",
              "        -0.03009246, -0.08709338, -0.0534225 ,  0.21861278, -0.47479319]),\n",
              " 'cutting': array([ 0.03926995, -0.3398514 ,  0.06923595, -0.07621982, -0.03107816,\n",
              "        -0.03181107,  0.16920992, -0.2475709 , -0.28152084,  0.10556879]),\n",
              " 'down': array([ 0.00813841,  0.30233909,  0.31521298,  0.40135452,  0.4975416 ,\n",
              "        -0.2655604 ,  0.40110508,  0.34138554,  0.31240783, -0.45353836]),\n",
              " 'emissions': array([-0.13872061, -0.2605745 , -0.46516951, -0.10718497, -0.33319318,\n",
              "        -0.47406992,  0.06624352,  0.14108718,  0.28154206, -0.42923539]),\n",
              " 'first': array([ 0.28538784, -0.20320656,  0.10640616, -0.44452564, -0.09295575,\n",
              "         0.45293436, -0.0609435 , -0.2817361 ,  0.30313451,  0.19179139]),\n",
              " 'frequently': array([ 0.26275913, -0.09892862, -0.19755593, -0.497932  ,  0.46227348,\n",
              "        -0.03169146,  0.31498203, -0.20096345,  0.19076472,  0.46863216]),\n",
              " 'fully': array([-0.11134535,  0.08063765, -0.13893233,  0.34909502,  0.25093102,\n",
              "         0.27987308,  0.48705613, -0.19791894,  0.13359126,  0.46821874]),\n",
              " 'if': array([-0.46550336, -0.11013139,  0.39881044, -0.2530156 ,  0.15737892,\n",
              "         0.05018722,  0.18329613,  0.09721837,  0.29094264,  0.21863686]),\n",
              " 'may': array([ 0.25733718, -0.24357251,  0.46871657,  0.27112475, -0.05407112,\n",
              "         0.13008155, -0.35424405,  0.01576083,  0.11876725, -0.17334077]),\n",
              " 'simply': array([ 0.31690681,  0.09526773,  0.01708527,  0.34990079,  0.36613129,\n",
              "        -0.17724488,  0.45351164, -0.40606465, -0.17445038, -0.19753906]),\n",
              " 'step': array([ 0.24661812,  0.1537155 ,  0.27978217,  0.23351629, -0.0689048 ,\n",
              "         0.40519681, -0.1426938 ,  0.27841499,  0.35906624, -0.03733413]),\n",
              " 'the': array([ 0.19588969, -0.45424147,  0.15219535, -0.27754848,  0.43619178,\n",
              "         0.37655274,  0.36016273, -0.25214214, -0.42676727,  0.472049  ]),\n",
              " 'to': array([ 0.43755793,  0.05840314,  0.40151608, -0.15713401, -0.06547864,\n",
              "        -0.34963405,  0.06150192, -0.06541241, -0.17673028,  0.25734168]),\n",
              " 'use': array([ 0.1652883 , -0.11451307,  0.44724012, -0.33026077, -0.34357872,\n",
              "         0.43785155, -0.34544327, -0.31887715,  0.29312754, -0.46160647]),\n",
              " 'well': array([-0.39092803,  0.05284109,  0.4959266 ,  0.25916429, -0.21780605,\n",
              "         0.45713859,  0.36206062,  0.283132  ,  0.13674026,  0.24776696]),\n",
              " 'you': array([-0.3443766 ,  0.00506749, -0.37284193,  0.2019557 , -0.02694693,\n",
              "         0.48346101,  0.28169044, -0.31172723, -0.33109064, -0.44835415]),\n",
              " 'your': array([-0.3752695 , -0.43242   ,  0.0386868 , -0.1074438 , -0.26541951,\n",
              "        -0.27381622, -0.18723846,  0.02966768,  0.44022048, -0.01452941])}"
            ]
          },
          "metadata": {},
          "execution_count": 303
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GPpKHURF3qNp"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNZYzxDXZgpcNMiV9yMS53C"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}