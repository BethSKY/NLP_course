{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tb3vk2XeKtT"
      },
      "source": [
        "# <font color=purple>Trying to understand how Word2Vec works</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "gHx3xnSATylQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bef5220-45a8-4e2a-980e-0623ec00aa63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "import gensim.downloader as api\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
        "import logging \n",
        "\n",
        "# nltk.download('stopwords') #<-- we run this command to download the stopwords in the project\n",
        "# nltk.download('punkt') #<-- essential for tokenization\n",
        "# nltk.download('gutenberg') #<-- corpus for training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "T7CLM5f2BLTx"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import itertools\n",
        "import math\n",
        "import string\n",
        "import nltk\n",
        "# Access the Gutenberg Corpus\n",
        "from nltk.corpus import gutenberg\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ku-rSdWzhn3S"
      },
      "source": [
        "# Function from lecturer\n",
        "## <font color=purple>The function that cleans the data</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "BveB75-ThpA4"
      },
      "outputs": [],
      "source": [
        "def clean(inp: str) -> str:\n",
        "\n",
        "    inp = inp.translate(str.maketrans(string.punctuation, \" \"*len(string.punctuation)))\n",
        "    inp = re.sub(r'\\s+', ' ', inp.lower())\n",
        "\n",
        "    return inp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "IQS0IwTdhuB9",
        "outputId": "5928e1ed-a91e-436d-a83f-4117bdbac222"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' if you use a car frequently the first step to cutting down your emissions may well be to simply fully consider the alternatives available to you '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# from a list of sentences\n",
        "documents = [\"If you use a car frequently, the first step to cutting\",\n",
        "             \"down your emissions may well be to simply\", \n",
        "             \"fully consider the\", \n",
        "             \"alternatives available to you.\"\n",
        "             ]\n",
        "clean(str(documents))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIXz6PtHhfjN"
      },
      "source": [
        "# <font color=purple>To train my own Word2Vec model I need:</font>\n",
        "1. **Obtain a large corpus of text**: You will need a large amount of text data to train your Word2vec model. You can obtain text data from various sources such as Wikipedia, news articles, or social media posts.\n",
        "2. **Preprocess the text**: Before training the Word2vec model, you will need to preprocess the text data by removing stop words, punctuations, and other non-essential elements. You can also tokenize the text data into words or phrases to prepare it for training.\n",
        "3. **Choose a Word2vec algorithm**: There are two main algorithms for training Word2vec models: Continuous Bag of Words (CBOW) and Skip-gram. CBOW predicts a word based on its context, while Skip-gram predicts the context based on a word. Choose the algorithm that best suits your needs.\n",
        "4. **Train the Word2vec model**: You can train the Word2vec model using popular libraries such as Gensim or TensorFlow. These libraries provide easy-to-use functions to train and test the Word2vec model. During training, the model learns to associate each word in the vocabulary with a vector of real numbers, which represent the word embedding.\n",
        "5. **Evaluate the Word2vec model**: Once the Word2vec model is trained, you can evaluate its performance using intrinsic or extrinsic evaluation methods. Intrinsic evaluation focuses on evaluating specific aspects of the model in isolation, such as its ability to generate embeddings or to classify sentences based on sentiment. Extrinsic evaluation measures the performance of the model on a specific task, such as language modeling or sentiment analysis.\n",
        "6. **Use the Word2vec model**: After training and evaluating the Word2vec model, you can use it for various natural language processing tasks, such as information retrieval, text classification, or machine translation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdWXu_HUkrJC"
      },
      "source": [
        "## <font color=blue>Obtain a large corpus of text</font>\n",
        "> Попробуем сначала поработать с корпусом Gutenberg (классическая литература), если что, переключимся на Brown (новости на английском)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIgUJOK4kx0M",
        "outputId": "2ff306ec-19d8-4311-c48a-6c16c3e5bda3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt'] \n",
            "\n",
            "['[', 'Alice', \"'\", 's', 'Adventures', 'in', 'Wonderland', 'by', 'Lewis', 'Carroll', '1865', ']']\n",
            "['CHAPTER', 'I', '.']\n",
            "['Down', 'the', 'Rabbit', '-', 'Hole']\n",
            "['Alice', 'was', 'beginning', 'to', 'get', 'very', 'tired', 'of', 'sitting', 'by', 'her', 'sister', 'on', 'the', 'bank', ',', 'and', 'of', 'having', 'nothing', 'to', 'do', ':', 'once', 'or', 'twice', 'she', 'had', 'peeped', 'into', 'the', 'book', 'her', 'sister', 'was', 'reading', ',', 'but', 'it', 'had', 'no', 'pictures', 'or', 'conversations', 'in', 'it', ',', \"'\", 'and', 'what', 'is', 'the', 'use', 'of', 'a', 'book', \",'\", 'thought', 'Alice', \"'\", 'without', 'pictures', 'or', 'conversation', \"?'\"]\n",
            "['So', 'she', 'was', 'considering', 'in', 'her', 'own', 'mind', '(', 'as', 'well', 'as', 'she', 'could', ',', 'for', 'the', 'hot', 'day', 'made', 'her', 'feel', 'very', 'sleepy', 'and', 'stupid', '),', 'whether', 'the', 'pleasure', 'of', 'making', 'a', 'daisy', '-', 'chain', 'would', 'be', 'worth', 'the', 'trouble', 'of', 'getting', 'up', 'and', 'picking', 'the', 'daisies', ',', 'when', 'suddenly', 'a', 'White', 'Rabbit', 'with', 'pink', 'eyes', 'ran', 'close', 'by', 'her', '.']\n",
            "['[', 'Alice', \"'\", 's', 'Adventures', 'in', 'Wonderland', 'by', 'Lewis', 'Carroll', '1865', ']', 'CHAPTER', 'I', '.', 'Down', 'the', 'Rabbit', '-', 'Hole']\n"
          ]
        }
      ],
      "source": [
        "# Print the categories in the Brown Corpus\n",
        "print(gutenberg.fileids(), '\\n')\n",
        "\n",
        "# Print the first few sentences of a book\n",
        "sentences = gutenberg.sents('carroll-alice.txt')\n",
        "for sentence in sentences[:5]:\n",
        "    print(sentence)\n",
        "\n",
        "# Transform data from nested lists to one list\n",
        "result1 = list(itertools.chain(*sentences))\n",
        "print(result1[:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjKWptxrqnbO"
      },
      "source": [
        "## <font color=blue>Preprocess the text</font>\n",
        "\n",
        "Source: https://towardsdatascience.com/how-to-train-a-word2vec-model-from-scratch-with-gensim-c457d587e031"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "47T2Jo2GlKQu"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text: str, remove_stopwords: bool) -> str:\n",
        "    \"\"\"Function that cleans the input text by going to:\n",
        "    - remove links\n",
        "    - remove special characters\n",
        "    - remove numbers\n",
        "    - remove stopwords\n",
        "    - convert to lowercase\n",
        "    - remove excessive white spaces\n",
        "    Arguments:\n",
        "        text (str): text to clean\n",
        "        remove_stopwords (bool): whether to remove stopwords\n",
        "    Returns:\n",
        "        str: cleaned text\n",
        "    \"\"\"\n",
        "    # remove links\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)\n",
        "    # remove numbers and special characters\n",
        "    text = re.sub(\"[^A-Za-z]+\", \" \", text)\n",
        "    # remove stopwords\n",
        "    if remove_stopwords:\n",
        "        # 1. create tokens\n",
        "        tokens = nltk.word_tokenize(text)\n",
        "        # 2. check if it's a stopword\n",
        "        tokens = [w.lower().strip() for w in tokens if not w.lower() in stopwords.words(\"english\")]\n",
        "        # return a list of cleaned tokens\n",
        "        return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "DTmZbA-wmLII",
        "outputId": "a071afdf-b369-4c98-d71e-e9aaf70789b6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                           sentences  \\\n",
              "0  ['[', 'Alice', \"'\", 's', 'Adventures', 'in', '...   \n",
              "1                              ['CHAPTER', 'I', '.']   \n",
              "2             ['Down', 'the', 'Rabbit', '-', 'Hole']   \n",
              "3  ['Alice', 'was', 'beginning', 'to', 'get', 've...   \n",
              "4  ['So', 'she', 'was', 'considering', 'in', 'her...   \n",
              "\n",
              "                                             cleaned  \n",
              "0    [alice, adventures, wonderland, lewis, carroll]  \n",
              "1                                          [chapter]  \n",
              "2                                     [rabbit, hole]  \n",
              "3  [alice, beginning, get, tired, sitting, sister...  \n",
              "4  [considering, mind, well, could, hot, day, mad...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c04452c6-c53f-4362-a5d9-e654c88d1cb7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentences</th>\n",
              "      <th>cleaned</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>['[', 'Alice', \"'\", 's', 'Adventures', 'in', '...</td>\n",
              "      <td>[alice, adventures, wonderland, lewis, carroll]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>['CHAPTER', 'I', '.']</td>\n",
              "      <td>[chapter]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>['Down', 'the', 'Rabbit', '-', 'Hole']</td>\n",
              "      <td>[rabbit, hole]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>['Alice', 'was', 'beginning', 'to', 'get', 've...</td>\n",
              "      <td>[alice, beginning, get, tired, sitting, sister...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>['So', 'she', 'was', 'considering', 'in', 'her...</td>\n",
              "      <td>[considering, mind, well, could, hot, day, mad...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c04452c6-c53f-4362-a5d9-e654c88d1cb7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c04452c6-c53f-4362-a5d9-e654c88d1cb7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c04452c6-c53f-4362-a5d9-e654c88d1cb7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "df = pd.DataFrame({'sentences': pd.Series(sentences).astype(str)})\n",
        "df['cleaned'] = df.sentences.apply(lambda x: preprocess_text(x, remove_stopwords=True))\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWHpgzmEpOGF",
        "outputId": "2b430e19-bc4b-404b-c5aa-efe4c056072c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['alice', 'adventures', 'wonderland', 'lewis', 'carroll'],\n",
              " ['chapter'],\n",
              " ['rabbit', 'hole']]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "texts = df.cleaned.tolist()\n",
        "texts[:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color=blue>Train the Word2Vec model</font>"
      ],
      "metadata": {
        "id": "gX40D4TRq0Yc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Word2Vec:\n",
        "    def __init__(self, window_size=2, vector_size=100, learning_rate=0.025, epochs=10):\n",
        "        self.window_size = window_size\n",
        "        self.vector_size = vector_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.word2id = {}\n",
        "        self.id2word = {}\n",
        "        self.W = None\n",
        "        self.loss_history = []\n",
        "\n",
        "    def _build_vocab(self, sentences):\n",
        "        words = []\n",
        "        for sentence in sentences:\n",
        "            words.extend(sentence)\n",
        "        words = sorted(set(words))\n",
        "        self.word2id = {w: i for i, w in enumerate(words)}\n",
        "        self.id2word = {i: w for w, i in self.word2id.items()}\n",
        "        \n",
        "    def _skipgrams(self, sentence):\n",
        "        pairs = []\n",
        "        for i, w in enumerate(sentence):\n",
        "            for j in range(i - self.window_size, i + self.window_size + 1):\n",
        "                if j != i and j >= 0 and j < len(sentence):\n",
        "                    pairs.append((w, sentence[j]))\n",
        "        print(pairs)\n",
        "        return pairs\n",
        "\n",
        "    def _sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        self.W = np.random.uniform(-0.5, 0.5, size=(len(self.word2id), self.vector_size))\n",
        "        self.W_context = np.random.uniform(-0.5, 0.5, size=(len(self.word2id), self.vector_size))\n",
        "\n",
        "    def _train(self, pairs):\n",
        "        for epoch in range(self.epochs):\n",
        "            loss = 0\n",
        "            for pair in pairs:\n",
        "                center_word = np.zeros((self.vector_size,))\n",
        "                context_word = np.zeros((self.vector_size,))\n",
        "                u = self.word2id[pair[0]]\n",
        "                v = self.word2id[pair[1]]\n",
        "                center_word = self.W[u]\n",
        "                for context in range(len(self.word2id)):\n",
        "                    if context == v:\n",
        "                        context_word = self.W_context[context]\n",
        "                        z = np.dot(center_word, context_word)\n",
        "                        sig = self._sigmoid(z)\n",
        "                        e = (1 - int(context == v)) - sig\n",
        "                        loss += e**2\n",
        "                        grad_sig = e * self.learning_rate\n",
        "                        grad_context_word = center_word * grad_sig\n",
        "                        grad_center_word = context_word * grad_sig\n",
        "                        self.W[u] -= grad_center_word\n",
        "                        self.W_context[context] -= grad_context_word\n",
        "            self.loss_history.append(loss / len(pairs))\n",
        "\n",
        "    def fit(self, sentences):\n",
        "        self._build_vocab(sentences)\n",
        "        pairs = []\n",
        "        for sentence in sentences:\n",
        "            pairs.extend(self._skipgrams(sentence))\n",
        "        self._initialize_weights()\n",
        "        self._train(pairs)\n",
        "        \n",
        "    def get_word_vector(self, word):\n",
        "        idx = self.word2id[word]\n",
        "        return {word: self.W[idx]}\n",
        "\n",
        "    def most_similar(self, word, n=5):\n",
        "        idx = self.word2id[word]\n",
        "        word_vector = self.W[idx]\n",
        "        sim = np.dot(self.W, word_vector)\n",
        "        closest = np.argsort(sim)[::-1][:n]\n",
        "        return [(self.id2word[idx], self.W[idx]) for idx in closest]\n",
        "        # return {word: vec for word, vec in a}\n"
      ],
      "metadata": {
        "id": "78bIsM0eOuLY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# создание объекта модели\n",
        "model = Word2Vec(window_size=2, vector_size=10, learning_rate=0.025, epochs=3)\n",
        "\n",
        "# обучение модели на нашем тексте\n",
        "model.fit(texts)"
      ],
      "metadata": {
        "id": "zsHMhORrOyn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.get_word_vector('hole')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_1-CQBIPBDO",
        "outputId": "2829b280-ce6e-430f-a7df-e2bf7526bc58"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'hole': array([ 105.06730346,  -38.44200512,  -30.30935   ,   29.11156463,\n",
              "        -159.95082517, -365.60176447, -192.61766741,  -69.12397709,\n",
              "         503.97663284,  635.90433587])}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(data: str):\n",
        "    \"\"\"\n",
        "    return: w2v_dict: dict\n",
        "            - key: string (word)\n",
        "            - value: np.array (embedding)\n",
        "    \"\"\"\n",
        "    return {}"
      ],
      "metadata": {
        "id": "91BOX1nSPWH8"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l1z3E28AtLeU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMsmhd5KsVQBTIbgJ3r+gBZ"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}